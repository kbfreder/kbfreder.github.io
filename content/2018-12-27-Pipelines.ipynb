{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Building flexible custom pipelines in scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "Some datasets are simple: they're entirely comprised of numeric features that all need to be scaled. Or maybe there are a few Categorical columns and your model doesn't require scaling of the numerical features. Rejoice, for your preprocessing will be easy.\n",
    "\n",
    "But what if you have a diverse dataset, or your preprocessing needs are more complex, or you want to preprocess your training and test data with a single line of code: this calls for exploiting scikit-learn's many preprocessing and pipeline functions. This blog post examines how (and when) to use `Pipeline` (or `make_pipeline`), `ColumnTransformer`, `FunctionTransformer`, and `FeatureUnion` to create custom transformation pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "The dataset that motivated this deep dive into the flexibility (and potential to confuse) of scikit-learn's customizable pipelines contained the following types of data:\n",
    "\n",
    "- Numerical\n",
    "- Categorical - two flavors:\n",
    "  - value could be 1 of several (ex: 'acid', or 'base')\n",
    "  - value could be 1 *or more* of several (ex: 'protein, nucleic acid', or 'tissue')\n",
    "- Boolean\n",
    "  - 1 = True, 0 = False\n",
    "- Engineered Features \n",
    "   - mathematical combinations of two or more other columns\n",
    "\n",
    "In addition, I wanted to test out Logistic Regression, tree-based models (Gradient Boosting Decision Trees), and CatBoost. Each required different preprocessing, but there were transformers in common between them. I opted to build up my pipelines using a \"unit\" approach. This made the final pipeline creation facile and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Players\n",
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the most commonly used and most familiar to Data Scientists, `Pipeline` processes data sequentially according to the defined steps. It passes the output of the first step into the second, and so on. The final step can be and often is an estimator (i.e. a model to fit your data against). The `make_pipeline` offers a less-verbose way of creating a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        pipe = Pipeline([('scale', StandardScaler()), ('model', LogisticRegression())])\n",
    "        same_pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ColumnTransformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful ally in the quest to create a custom pipeline is `ColumnTransformer`. You feed it a list of transformers, which themselves are tuples of a user-defined transformer name, the transformer function, and the column indices to transform. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('scaler', StandardScaler(), [2,4,6,8]),\n",
    "    ('encoder', OneHotEncoder(), [1,3,5])\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the flexibility to apply transformers to a subset of columns in your dataframe. You can specify whether to pass-through or drop the remaining columns. If you only need to transform a subset of your columns, using `ColumnTransformer` with `remainder=passthrough` is an efficient way to do so, as shown in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FeatureUnion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FeatureUnion` takes a list of transformers as an argument (technically tuples of the format: `(<name>, <transformer>)`), performs them separately on the data, and concatenates the outputs together. Using the (default) `remainder=drop` option of `ColumnTransformer` coupled with `FeatureUnion` is akin to the split-apply-combine dogma of groupby & apply/aggregate. This can be useful when you want to define transformations piece-by-piece and formulate different combinations of them depending on model requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = FeatureUnion([\n",
    "        ('some_cols', custom_transformer_01),\n",
    "        ('other_cols', custom_transformer_02)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom functions and classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FunctionTransformer` allows us to use an \"arbitrary callable\", such as a simple function, as a preprocessing transformer. This is useful for stateless transformations, like taking the log of something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(X):\n",
    "    return X * 2\n",
    "\n",
    "preproc_cv = Pipeline([\n",
    "    ('doubler', FunctionTransformer(double, validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Pipeline Pieces\n",
    "Now that we've introduced the scikit-learn functions, let's return to our data and begin to construct a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other functions and packages we'll need to load\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Variables with >1 possible value**\n",
    "\n",
    "For the categorical variables that could take on more than one value, I use the `CountVectorize` function. (See <a href=\"https://kbfreder.github.io/2018-12-12-Small-Molecules.html#2018-12-12-Small-Molecules\" target=\"_blank\">this blog post</a> for details on how this function is used as a preprocessor for this data.) One column, *target_classes*, first needed to have missing values imputed. I create a custom class for this, which returns only the column of interest (this is analogous to setting `remainder=drop`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imputation class\n",
    "class Imputer(object):\n",
    "    '''Fills in missing (na) values in 'col_name' column with 'imp_val' '''\n",
    "    def __init__(self, col_name, imp_val=''):\n",
    "        self.col_name = col_name\n",
    "        self.imp_val = imp_val\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X[self.col_name].fillna(value=self.imp_val)\n",
    "\n",
    "# define transformer function: CountVectorize with a custom token_pattern\n",
    "target_transformer = CountVectorizer(token_pattern=r'([\\w*-]{1,}),*')\n",
    "\n",
    "# define preprocessor \n",
    "preproc_target = Pipeline([\n",
    "        ('impute', Imputer('target_classes')),\n",
    "        ('target_cv', target_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other column, *assay_types*, can be Count-Vectorized directly with a custom token pattern. Note that the `CountVectorize` function requires that the column indices be provided as a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the column indices to transform\n",
    "asy_cls_col = list(X_train.columns).index('assay_types')\n",
    "\n",
    "# define transformer function: CountVectorize with a custom token_pattern\n",
    "assay_transformer = CountVectorizer(lowercase=False, token_pattern=r'\\[*(\\w{1}),*\\]*', )\n",
    "\n",
    "# define preprocessor\n",
    "preproc_assay = ColumnTransformer(transformers=[\n",
    "            ('assay_cv', assay_transformer, asy_cls_col)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I sew these two steps together into one preprocessor (named 'cv' for CountVectorize) using `Pipeline` and `FeatureUnion`. Recall that: `Pipeline` processes data sequentially, passing the output of the first step into the second, and so on, and `FeatureUnion` performs the transformations separately, then concatenates the results together. \n",
    "\n",
    "Note that the output of `CountVectorize` is a sparse (CSR) matrix; some of the models I plan to evaluate require a dense matrix, so I convert the output to an *ndarray* using a user-defined function, *densify*, and `FunctionTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify(X):\n",
    "    return X.toarray()\n",
    "\n",
    "preproc_cv = Pipeline([\n",
    "    ('cv', FeatureUnion([\n",
    "        ('assays', preproc_assay),\n",
    "        ('targets', preproc_target)\n",
    "        ])),\n",
    "    ('densify', FunctionTransformer(densify, validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical variables with only 1 possible value**\n",
    "\n",
    "More typically, categorical variables can only take on one value, and thus we can use scikit-learn's `OneHotEncode` function to encode this information. It's worth noting that the newer algorithm CatBoost <a href=\"https://tech.yandex.com/catboost/doc/dg/features/categorical-features-docpage/#dataset-processing\"Â target=\"_blank\">does not require</a> encoding of single-value categorical variables prior to feeding the data into the model.\n",
    "\n",
    "I perform this preprocessing step using `ColumnTransformer` so I can specify which columns to encode. Note that in contrast to `CountVectorize` above, `OneHotEncode` requires that columns indices be specified in list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical columns to encode & make a list of their indices\n",
    "cols_to_encode = ['molecular_species']\n",
    "cols_to_encode_idx = [list(X_train.columns).index(x) for x in cols_to_encode]\n",
    "\n",
    "# define preprocessor\n",
    "preproc_ohe = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(), cols_to_encode_idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numeric features**\n",
    "\n",
    "Logistic Regression requires (sometimes -- see <a href='#footnote-01'>Footnote 1</a>) numerical data to be scaled. I again use `ColumnTransformer` to specify which columns to transform with `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to scale & make a list of their indices.\n",
    "cols_to_scale = ['mw_freebase', 'alogp', 'acd_logp', 'acd_logd', 'hba', 'hbd', 'psa', 'rtb', \n",
    "       'num_ro5_violations', 'aromatic_rings', 'heavy_atoms', 'hba_lipinski', 'hbd_lipinski',  \n",
    "       'num_target_organisms', 'num_alerts_set1']\n",
    "cols_to_scale_idx = [list(X_train.columns).index(x) for x in cols_to_scale]\n",
    "\n",
    "# apply preprocessor\n",
    "preproc_scale = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), cols_to_scale_idx), \n",
    "    ], remainder='drop')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features to leave un-touched**\n",
    "\n",
    "These features are already Boolean-encoded, where 1 = True and 0 = False. They will need to be identified as categorical features in CatBoost. I again use `ColumnTransformer` and instead of specifying a transformer, I use the special string `passthrough` to indicate that this is what I want done with these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns and a list of their indices\n",
    "cols_to_pass = ['ro3_pass', 'research_co','human_target']\n",
    "cols_to_pass_idx = [list(X_train.columns).index(x) for x in cols_to_pass]\n",
    "\n",
    "# define preprocessor\n",
    "preproc_pass = ColumnTransformer(transformers=[\n",
    "        ('as_is', 'passthrough', cols_to_pass_idx), \n",
    "])\n",
    "\n",
    "# for tree-based models, also want to pass through numeric columns\n",
    "preproc_pass_num = ColumnTransformer(transformers=[\n",
    "        ('as_is', 'passthrough', cols_to_scale_idx), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Engineered Features**\n",
    "\n",
    "While creating engineered features based on mathematical combinations of other columns could be done directly on the dataframe, I am in the habit of splitting off my test set immediately, and the decision to engineer features often comes after EDA (exploratory data analysis). Thus, I wanted to include feature engineering in my pipeline, so my test set (and any new data) could be transformed as part of the same preprocessing pipeline. \n",
    "\n",
    "I define custom classes to perform these transformations: an average and a ratio. There is the possibility that in my ratio calculation the denominator is 0, meaning the ratio is undefined. I use `fillna(0)` to combat this. You'll notice that I essentially compute the average twice: once in the *TargetActivityAvg* class transform, and again in the *AssayRatio* class transform. Try as I might, I couldn't contrive a pipeline that would enable me to pass both the calculated average and the existing dataframe column *num_assays* required for the ratio calculation. As the average calculation is simple enough, I repeat it. But a smarter person can probably figure out a way not to. \n",
    "\n",
    "Note these transformers take the entire dataframe as input and return a single pandas Series as output. This needs to be converted to a numpy array before combing back together with the rest of the transformed features, hence the usage of the simple function *reshaper* and `FunctionTransformer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom classes & function\n",
    "class TargetActivityAvg(object):\n",
    "    '''Returns average of num_activities & num_targets columns.'''\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['num_activities'] + X['num_targets']) / 2\n",
    "\n",
    "\n",
    "class AssayRatio(object):\n",
    "    '''Returns ratio of num_assays / avg(num_targets & num_activities).'''\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        avg = (X['num_activities'] + X['num_targets']) / 2\n",
    "        ratio =  X['num_assays'] / avg\n",
    "        ratio = ratio.fillna(0)\n",
    "        return ratio\n",
    "\n",
    "def reshaper(X):\n",
    "    return X.values.reshape(-1,1)\n",
    "\n",
    "# define feature engineering preprocessors\n",
    "feat_eng1 = Pipeline([\n",
    "    ('avg', TargetActivityAvg()),\n",
    "    ('reshape', FunctionTransformer(reshaper, validate=False))\n",
    "])\n",
    "\n",
    "feat_eng2 = Pipeline([\n",
    "        ('ratio',  AssayRatio()),\n",
    "        ('reshape', FunctionTransformer(reshaper, validate=False)),\n",
    "        ])\n",
    "\n",
    "# combine them into one preprocessor\n",
    "preproc_feat_eng = Pipeline([\n",
    "    ('feat_eng', FeatureUnion([\n",
    "        ('avg', feat_eng1),\n",
    "        ('ratio', feat_eng2)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# need to scale these engineered features for Logistic Regression\n",
    "preproc_feat_eng_scaled = Pipeline([\n",
    "    ('feat_eng', FeatureUnion([\n",
    "        ('avg', feat_eng1),\n",
    "        ('ratio', feat_eng2)\n",
    "    ])),\n",
    "    ('scale', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them all together\n",
    "\n",
    "The final step is to combine the various preprocessors together into a single pipeline. Because the transformer steps were defined separately, they can be combined together in different ways, depending on the downstram application (model). \n",
    "\n",
    "**Scaling**: This pipeline scales the numerical features, e.g. for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_with_scale = Pipeline([\n",
    "    ('all', FeatureUnion([\n",
    "        ('cvs', preproc_cv),\n",
    "        ('feat_eng', preproc_feat_eng_scaled),\n",
    "        ('ohe', preproc_ohe), \n",
    "        ('pass', preproc_pass),\n",
    "        ('num', preproc_scale),\n",
    "    ])\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical only**: This pipeline is for models that don't require scaling, e.g. Gradient Boosting Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cat_only = Pipeline([\n",
    "    ('all', FeatureUnion([\n",
    "        ('cvs', preproc_cv),\n",
    "        ('feat_eng', preproc_feat_eng),\n",
    "        ('ohe', preproc_ohe), \n",
    "        ('pass', preproc_pass),\n",
    "        ('num', preproc_pass_num)\n",
    "    ])\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**: As described above, CatBoost has different preprocessing needs (no One-Hot Encoding). The last bit of code below gives the columns indices for the categorical features, which need to be provided to the CatBoost algorithm. Note that I had to determine these by hand -- I couldn't figure out a way to do it automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns that do not require any preprocessing\n",
    "other_cb_features = ['mw_freebase', 'alogp', 'acd_logp', 'acd_logd', 'hba', 'hbd', 'psa', 'rtb', 'ro3_pass',\n",
    "       'num_ro5_violations', 'molecular_species', 'aromatic_rings', 'heavy_atoms', 'qed_weighted',\n",
    "       'hba_lipinski', 'hbd_lipinski', 'research_co', 'num_target_organisms',\n",
    "        'human_target', 'num_target_organisms', 'num_alerts_set1']\n",
    "other_cb_feat_idx = [list(X_train.columns).index(x) for x in other_cb_features]\n",
    "\n",
    "# create a transformer to pass them through\n",
    "preproc_pass_cb = ColumnTransformer(transformers=[\n",
    "        ('as_is', 'passthrough', other_cb_feat_idx), \n",
    "])\n",
    "\n",
    "# full pipeline\n",
    "pipe_cat_boost = Pipeline([\n",
    "    ('all', FeatureUnion([\n",
    "        ('cvs', preproc_cv),\n",
    "        ('feat_eng', preproc_feat_eng),\n",
    "        ('pass', preproc_pass_cb)\n",
    "    ])\n",
    "    )\n",
    "])\n",
    "\n",
    "# where the \"categorical\" columns end up after the above pre-processing\n",
    "cab_cat_feats = ['ro3_pass', 'molecular_species', 'research_co', 'human_target']\n",
    "cb_cat_cols = [20, 22, 28, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline in action\n",
    "Here is a brief example of how to use one of the preprocessors (*pipe_with_scale*) in combination with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_pipe = make_pipeline(pipe_with_scale, LogisticRegression())\n",
    "log_reg_pipe.fit(X_train, y_train)\n",
    "y_pred = log_reg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Extracting feature names\n",
    "Often, after fitting your model, you want to extract feature importances. Which features are impacting your model greatly? Which have little to no impact (and thus could be dropped)? The below code keeps track of and extracts the final order of features in the various pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_assay.fit(X_train)\n",
    "feat_cv_asy = preproc_assay.named_transformers_['assay_cv'].get_feature_names()\n",
    "\n",
    "preproc_target.fit(X_train)\n",
    "feat_cv_trg = preproc_target.named_steps['target_cv'].get_feature_names()\n",
    "\n",
    "feat_cv_asy = ['assay_class_' + x for x in feat_cv_asy]\n",
    "feat_cv_trg = ['target_class_' + x for x in feat_cv_trg]\n",
    "\n",
    "preproc_ohe.fit(X_train)\n",
    "feat_ohe = preproc_ohe.named_transformers_['cat'].get_feature_names()\n",
    "feat_ohe = feat_ohe.tolist()\n",
    "\n",
    "feat_num = cols_to_scale\n",
    "\n",
    "feat_pass = cols_to_pass\n",
    "\n",
    "feat_fe = ['avg_num_targ_act', 'ratio_assay_avg_targ_act']\n",
    "\n",
    "# Feature names for pipe_cat_only & pipe_with_scale\n",
    "feat_names = feat_cv_asy + feat_cv_trg + feat_fe + feat_ohe + feat_pass + feat_num\n",
    "\n",
    "# for pipe_cat_boost:\n",
    "feat_other_cb = other_cb_features\n",
    "feat_names_cb = feat_cv_asy + feat_cv_trg + feat_fe + feat_other_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In summary, we've created 3 different pipelines, built the same \"units\" or base transformers. Creating small units separately and subsequently combining them in different ways allows for flexibility and efficiency. Defining custom transformer classes and functions expands the functionality of scikit-learn's builtin capabilities. `ColumnTransformer`, `FeatureUnion`, `FunctionTransformer`, and of course, `Pipeline` are powerful tools in developing custom preprocessors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Footnotes:\n",
    "1. <a id='footnote-01'></a>Apparently, and I was unaware of this before I began writing this blog post, certain Logistic Regression solvers are robust to unscaled data, including the one I used ('liblinear'). See the summary table at https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
